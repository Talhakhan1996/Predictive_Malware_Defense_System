# Importing Packages
# import pefile
# import pip
# import sklearn
import pickle
# from tensorflow import keras

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# from keras.applications.densenet import layers
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier
import joblib
# from sklearn import tree, linear_model
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import confusion_matrix
# from sklearn.datasets import make_classification
# import tensorflow as tf
# from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# print(tf.__version__)

# Importing the dataset
data = pd.read_csv("MalwareData.csv", sep='|')

leg = data[0:41323].drop(['legitimate'], axis=1)
mal = data[41323::].drop(['legitimate'], axis=1)

print("Size of legitimate files are: %i samples, %i features" % (leg.shape[0], leg.shape[1]))
print("Size of Malware files are: %i samples, %i features" % (mal.shape[0], mal.shape[1]))
print("Total important samples are: %i samples" % data.shape[0], '\n')

# fig = plt.figure()
# ax = fig.add_axes([0,0,1,1])
# ax.hist(data['legitimate'],20)
# plt.show()

print(data.columns, '\n')
pd.set_option("display.max_columns", None)
print(data.head, '\n')
print(leg.take, '\n')
print(mal.take, '\n')

# Coding and Feature Selection using tree Classifier
x = data.drop(['Name', 'md5', 'legitimate'], axis=1).values
y = data['legitimate'].values
classifier = ExtraTreesClassifier().fit(x, y)
model = SelectFromModel(classifier, prefit=True)
x_new = model.transform(x)
print(np.shape(x_new))
print(x.shape, x_new.shape, '\n')
features = x_new.shape[1]
important = classifier.feature_importances_

# Splitting the dataset into the Training set and Test set
x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2)
print('features identified as important:')
indices = np.argsort(important)[::-1]

for f in range(features):
    print("%d. feature %s (%f)" % (f + 1, data.columns[2 + indices[f]], important[indices[f]]), "\n")

# Classifier Comparison
dtc = DecisionTreeClassifier(max_depth=10)
dtc.fit(x_train, y_train)
print("The DecisionTree score is: ", dtc.score(x_test, y_test) * 100)
y_pred = dtc.predict(x_test)
mt = confusion_matrix(y_test, y_pred)
print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0]))) * 100))
print('False negative rate : %f %%' % ((mt[1][0] / float(sum(mt[1]))) * 100))
g = pd.DataFrame(mt)
g
plt.title('Confusion matrix of Decision Tree')
print(mt)
mal = sns.heatmap(mt, annot=True, linewidths=1, cmap="Blues", linewidth=1, linecolor='k')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

rfc = RandomForestClassifier(n_estimators=50)
rfc.fit(x_train, y_train)
print("The RandomForest score is: ", rfc.score(x_test, y_test) * 100)
y_pred = rfc.predict(x_test)
mt = confusion_matrix(y_test, y_pred)
print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0]))) * 100))
print('False negative rate : %f %%' % ((mt[1][0] / float(sum(mt[1]))) * 100))
g = pd.DataFrame(mt)
g
plt.title('Confusion matrix of Random Forest')
print(mt)
mal = sns.heatmap(mt, annot=True, linewidths=1, cmap="Blues", linewidth=1, linecolor='k')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

gba = GradientBoostingClassifier(n_estimators=100)
gba.fit(x_train, y_train)
print("The GradientBoosting score is: ", gba.score(x_test, y_test) * 100)
y_pred = gba.predict(x_test)
mt = confusion_matrix(y_test, y_pred)
print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0]))) * 100))
print('False negative rate : %f %%' % ((mt[1][0] / float(sum(mt[1])) * 100)))
g = pd.DataFrame(mt)
g
plt.title('Confusion matrix of GradientBoosting')
print(mt)
mal = sns.heatmap(mt, annot=True, linewidths=1, cmap="Blues", linewidth=1, linecolor='k')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

algorithms = {
    "DecisionTree": DecisionTreeClassifier(max_depth=10),
    "RandomForest": RandomForestClassifier(n_estimators=50),
    "GradientBoosting": GradientBoostingClassifier(n_estimators=100)
}

results = {}
print("\nNow testing algorithms")
for algo in algorithms:
    classifier = algorithms[algo]
    classifier.fit(x_train, y_train)
    score = classifier.score(x_test, y_test)
    print("%s : %f %%" % (algo, score * 100))
    results[algo] = score

winner = max(results)
print('\nWinner algorithm is %s with a %f %% success' % (winner, results[winner] * 100))

classifier = algorithms[winner]
y_pred = classifier.predict(x_test)
mt = confusion_matrix(y_test, y_pred)
print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0]))) * 100))
print('False negative rate : %f %%' % ((mt[1][0] / float(sum(mt[1]))) * 100))
g = pd.DataFrame(mt)
g
plt.title('Confusion matrix of Winner Algo')
print(mt)
mal = sns.heatmap(mt, annot=True, linewidths=1, cmap="Blues", linewidth=1, linecolor='k')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

# Neural Network
model = Sequential([
    Dense(32, activation='relu', input_shape=(len(x_new[1]),)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid'),
])
model.summary()

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
print(x_train.shape)
model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=1)

# Accuracy On training and Testing Models
model.evaluate(x_test, y_test)[1]
model.evaluate(x_train, y_train)[1]

scores = model.evaluate(x_train, y_train)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1] * 100))
scores = model.evaluate(x_test, y_test)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1] * 100))

gba.fit(x_test, y_test)[1]
gba.score(x_test, y_test)

gba.fit(x_train, y_train)[1]
gba.score(x_train, y_train)

rfc.fit(x_test, y_test)[1]
rfc.score(x_test, y_test)

rfc.fit(x_train, y_train)[1]
rfc.score(x_train, y_train)

dtc.fit(x_test, y_test)
dtc.score(x_test, y_test)

dtc.fit(x_train, y_train)
dtc.score(x_train, y_train)

gba.fit(x_train, y_train)
score = gba.score(x_train, y_train)
print("%s : %f %%" % (gba, score * 100))

gba.fit(x_test, y_test)
score = gba.score(x_test, y_test)
print("%s : %f %%" % (gba, score * 100))

dtc.fit(x_train, y_train)
score = dtc.score(x_train, y_train)
print("%s : %f %%" % (dtc, score * 100))

dtc.fit(x_test, y_test)
score = dtc.score(x_test, y_test)
print("%s : %f %%" % (dtc, score * 100))

rfc.fit(x_train, y_train)
score = rfc.score(x_train, y_train)
print("%s : %f %%" % (rfc, score * 100))

rfc.fit(x_test, y_test)
score = rfc.score(x_test, y_test)
print("%s : %f %%" % (rfc, score * 100))

# Save the algorithm and the feature list for later predictions
print('Saving algorithm and feature list in classifier directory...')

# with open('Classifier/classifier.pkl', 'wb') as pickle_file:
#     pickle.dump(classifier.score, pickle_file)
#
# with open('Classifier/classifier.pkl', 'rb') as pickle_file:
#     new_data = pickle.load(pickle_file)

joblib.dump(algorithms[winner], 'Classifier/classifier.pkl')
open('Classifier/features.pkl', 'wb').write(pickle.dumps(features))
print('Saved')
